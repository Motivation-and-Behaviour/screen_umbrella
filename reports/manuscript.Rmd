---
title: "Benefits and risks associated with children’s and adolescents’ interactions with electronic screens: An umbrella review"
author:
  - name: Taren Sanders
    affiliation: Institute for Positive Psychology and Education, Australian Catholic
      University
    affiliation_url: https://www.acu.edu.au/research/our-research-institutes/institute-for-positive-psychology-and-education
    orcid_id: 0000-0002-4504-6008
  - name: Philip Parker
    affiliation: Institute for Positive Psychology and Education, Australian Catholic
      University
    affiliation_url: https://www.acu.edu.au/research/our-research-institutes/institute-for-positive-psychology-and-education
    orcid_id: 0000-0002-4604-8566
  - name: Michael Noetel
    affiliation: School of Health and Behavioural Sciences, Australian Catholic University
    affiliation_url: https://www.acu.edu.au/about-acu/faculties-directorates-and-staff/faculty-of-health-sciences/school-of-behavioural-and-health-sciences
    orcid_id: 0000-0002-6563-8203
  - name: Chris Lonsdale
date: '`r format(Sys.Date())`'
repository_url: https://github.com/Motivation-and-Behaviour/screen_umbrella
bibliography: combined.bib
csl: nature.csl
params:
  render_img: true
  inc_ref: false
---

```{r setup, include=FALSE}
# This allows distill to render from a targets pipeline
Sys.setenv("RSTUDIO_VERSION" = '1.4.1725')

knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)

# Uncomment to run outside of pipeline
# library(tidyverse)
# library(targets)
# library(xfun)
```

```{r, load-targets}
targets::tar_load(c("prisma", "effects_clean", "reviews_tables", "tables_df",
                    "plots", "studies_results", "comparison_plots", 
                    "combined_effects", "combined_bib"), 
         store = here::here("_targets"))
```

# Background

In the 16th century, hysteria reigned around a new technology that threatened to be "confusing and harmful" to the mind. The cause of such concern? The widespread availability of books brought about by the invention of the printing press [@blairReadingStrategiesCoping2003]. In the early 19th century, concerns about schooling "exhausting the children's brains" followed, with the medical community accepting that excessive study could be a cause of madness [@bell1883sanitarian]. By the 20th century, the invention of the radio was accompanied by assertions that it would distract children from their reading (which by this point was no longer considered "confusing and harmful") leading to impaired learning [@dill2013oxford].

Today, the same arguments that were once leveled against reading, schooling, and radio are being made about screen time (e.g., TV or computers) [@wartellaChildrenComputersNew2000]. Excessive screen time is the number one concern parents have about their children's health and behaviour, ahead of nutrition, bullying, and physical inactivity [@rhodes2015top]. Yet, the evidence to support parents' concerns is lacking. A 2019 Lancet editorial [@thelancetSocialMediaScreen2019a] suggested that , "Our understanding of the benefits, harms, and risks of our rapidly changing digital landscape is sorely lacking."

While some forms of screen time (e.g., TV viewing) may be detrimental to behaviour and health [@haleScreenTimeSleep2015a;@sweetserActivePassiveScreen2012a], evidence for other forms of screen exposure (e.g., video games or online communication, such as Zoom) remains less certain and, in some cases, may even be beneficial [@liEarlyChildhoodComputer2004a;@warburton2017children]. Thus, efforts to determine the effect of screen exposure on youth is "a defining challenge of the digital age" [@orbenAssociationAdolescentWellbeing2019]. With concerns over the impact of screen time including education, health, and psychological well-being, a broad overview that acknowledges screen time's potential benefits and risks is needed.

Citing the negative effects on health (e.g., increased risk of obesity) and health-related behaviours (e.g., sleep), guidelines from numerous government agencies [@australiangovernmentPhysicalActivityExercise2021;@Canadian24HourMovement2016] and statements by expert groups [@AAPMediaUseSchoolAged2016;@whoGuidelinesPhysicalActivity2019] have recommended that young people's time spent using electronic media devices for entertainment purposes should be limited. For example, the Australian Government guidelines regarding sedentary behaviour recommend that young children (under the age of two) should not spend any time watching screens. They also recommend that children aged 2-5 years should spend a maximum of one hour exposed to recreational sedentary screen time per day, while children aged 5-12 and adolescents should spend no more than two hours. In contrast, some recent evidence suggests that exposure to electronic entertainment media that exceeds these guidelines (e.g., 3-4 hours per day) may not have meaningful adverse effects on children's behaviour or mental health [@fergusonEverythingModerationModerate2017], and might, in fact, benefit their well being, as long as this exposure does not reach extreme levels (e.g., 7 hours per day). Some research also indicates that content (e.g., video game vs television program) may also play an important role in determining the potential benefit or harm of youths' exposure to screen-based media [@przybylskiLargeScaleTestGoldilocks2017]. Indeed, educational screen time has been found to be positively related to educational outcomes [@sandersTypeScreenTime2019]. This evidence has led some researchers to argue that a more nuanced approach to screen time guidelines is required.

In 2016, the American Academy of Pediatrics used a narrative review examining the benefits and risks of children and adolescents' electronic media [@chassiakosChildrenAdolescentsDigital2016] as a basis for updating their guidelines [@AAPMediaUseSchoolAged2016]. Since then, a large number of systematic reviews and meta-analyses have provided up-to-date evidence about the potential benefits and risks of screentime. No review has synthesised the evidence available across relevant outcomes, such physical health, health behaviours, education, and psychological well being.

In order to support further evidence-based guideline development and refinement, we reviewed published meta-analyses examining the effects of screen time on young children, children, and adolescents. This review includes the association between electronic media use and any variable that is a plausible outcome of this exposure. Adopting this broad approach allowed us to provide a holistic perspective on the influence of screens on children's lives. By synthesising across life domains, this review provides evidence to support more targeted guidelines and advice for parents, teachers, and other professionals in order to maximise human functioning.

\newpage

# Methods

### Eligibility criteria

*Population*: To be eligible for inclusion, meta-analyses needed to include meta-analytic effect sizes for children or adolescents ( age 0-18 years). Meta-analyses containing data from adults and youth were included if meta-analytic effect sizes estimates specific to participants aged 18 years or less could be extracted. We excluded meta-analyses that only contained evidence gathered from adults (age >18 years).

*Exposure*: We included meta-analyses examining all types of electronic screens including (but not necessarily limited to) television, gaming consoles, computer, tablet, TV, and mobile phones. We also included analyses of all types of content on these devices, including (but not necessarily limited to) recreational content  (e.g., television programs, movies, games), homework, and communication (e.g., video chat). In this review we adopted a population-level perspective, meaning that we examined electronic media exposure that occurs during typical daily living activities (e.g., home, school-based electronic media exposure). Consistent with this population-level approach, we excluded technology-based treatments for clinical conditions. 

*Outcomes*: We included all reported outcomes.

*Publications*: We included meta-analyses (or meta-regressions) of quantitative evidence. To be included, meta-analyses needed to analyse data from studies identified in a systematic review. For our purposes, a systematic review is a review in which the authors attempted to acquire all the research evidence that pertained to their research question(s). We excluded meta-analyses that did not attempt to summarise all the available evidence (e.g., a meta-analysis of all studies from one laboratory). We included meta-analyses regardless of the study designs included in the review (e.g., laboratory-based experimental studies, randomised controlled trials. non-randomised controlled trials, longitudinal, cross-sectional, case studies), as long as the studies in the review collected quantitative evidence. We excluded systematic reviews of qualitative evidence. We did not formulate inclusion/exclusion criteria related to the risk of bias of the review. We did, however, employ a risk of bias tool to help us interpret the results. We included full-text, peer-reviewed meta-analyses published or ‘in-press’ in English. We excluded conference abstracts and meta-analyses that were unpublished.

### Information sources

We searched records contained in the following databases: Pubmed, MEDLINE, CINAHL, PsycINFO, SPORTDiscus, Education Source, Embase, Cochrane Library, Scopus, Web of Science, ProQuest Social Science Premium Collection, and ERIC. We conducted an initial search on August 17, 2018 and refreshed the search on May 13, 2020. We searched reference lists of included papers in order to identify additional eligible meta-analyses. We also searched PROSPERO to identify relevant protocols and contacted authors to determine if these reviews have been completed and published.

### Search strategy

The search strategy associated with each of the 12 databases can be found [here](https://docs.google.com/document/d/1hz5Dgw0aVOMeXL3vpRXsNtCIbf6dHZTb8uz7wQ29ke4/edit#heading=h.i6znptfz9nwa). We hand searched reference lists from any relevant umbrella reviews to identify systematic meta-analyses that our search may have missed.

### Selection process

Using Covidence software (Veritas Health Innovation, Melbourne, Australia), two researchers independently screened all titles and abstracts. Two researchers then independently reviewed full-text articles. We resolved disagreements at each stage of the process by consensus, with a third researcher employed, when needed.

### Data collection process
Two researchers independently extracted data from the included meta-analyses into a custom-designed database. 

### Data items

From each meta-analysis we extracted the following items: First author, year of publication, earliest and latest study publication dates, sample age, lowest and highest mean age reported, study design restrictions (e.g., cross-sectional, observational, experimental), region restrictions (e.g., specific countries), exposures reported, and outcomes reported.

### Study risk of bias assessment

For each meta-analysis, two researchers independently completed the National Health, Lung and Blood Institute’s Quality Assessment of Systematic Reviews and Meta-Analyses tool [@NHLBIQualityAssessmentSystematic2014]. We resolved disagreements by consensus, with a third researcher employed, when needed. We did not assess risk of bias in the individual studies that were included in each meta-analysis.

### Effect measures

Two researchers independently extracted all quantitative meta-analytic effect sizes, including moderation results. Where possible, they also extracted effect sizes from primary studies included in each meta-analysis. To facilitate comparisons, we converted effect sizes to Pearson’s $r$ using established formulae [@bonettTransformingOddsRatios2007;@bowmanEffectSizesStatistical2012;@jacobsEstimationBiserialCorrelation2017]. We excluded relative risk ratios from this conversion because meta-analyses did not contain sufficient information to do so. Effect sizes on the original metric are provided in supplementary materials.

### Synthesis methods

After extracting data, we examined the combinations of exposure and outcomes and removed any effects that appeared more than once, keeping the effect with the largest total sample size. We excluded effect size estimates when the authors did not provide a sample size. We descriptively present the remaining meta-analytic effect sizes. When the meta-analysis’s authors provided primary study data associated with these effects we reran the effect size estimate using a random effects meta-analysis via the metafor package [@R-metafor] in R [@R-base] (version `r paste(R.Version()[c("major", "minor")], collapse = ".")`). When required, we imputed missing sample sizes using mean imputation from the other studies within that review. From our reanalysis we also extracted I2 values. To test for publication bias, we also conducted Egger's test [@eggerBiasMetaanalysisDetected1997] where the number of studies within the review was ten or more [@pageChapter13Assessing2021], and conducted a test of excess significance [@ioannidisExploratoryTestExcess2007]. We contacted authors who did not provide primary study data in their published article. Where authors did not provide data in a format that could be re-analysed, we used the published results of their original meta-analysis.

### Evidence assessment criteria

*Statistical Credibility*. We employed a statistical classification approach to grade the statistical credibility of the effect sizes in the literature. To be considered credible an effect needed to be derived from a combined sample of >1,000 and have non-significant tests of publication bias (i.e., Egger’s test and excess significance test). We performed these analyses, and tehrefore the review needed to provide usable study-level data in order to be included.

*Consistency of Effect within the Population*. We also examined the consistency of the effect size using the $I^2$ measure. We considered $I^2 < 50\%$ to indicate effects that were relatively consistent across the population of interest. $I^2$ values of $> 50\%$ were taken to indicate an effect was potentially heterogeneous within the population.

*Direction of Effect*. Finally, we examined the extent to which significance testing suggested screen exposure was associated with benefit, harm, or no effect on outcomes. We used thresholds of $P < .05$ for weak evidence and $P < 10^{-3}$ for strong evidence. An effect that was neither significant at $P < .05$ or $10^{-3}$ that also passed the criteria for statistical credibility was taken to indicate no association of interest.

### Deviations from protocol

We initially planned to include systematic reviews without meta-analyses in a narrative summary alongside the main meta-analytic findings. However, we determined that combining results from the meta-analyses provided a robust overview. Readers interested in the relevant systematic reviews (i.e., without meta-analysis) can consult the list of references in eSupplementary File X. 

Philip.Parker@acu.edu.au will insert an explanation about why he forced us to move away from our original [evidence assessment plan](https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=76051).

# Results

### Search Results

```{r results}
rename_effects <- function(cur_name) {
  if (str_detect(cur_name, ":")) {
      
  cur_name_split <- str_split(cur_name, ": ")
  return(str_to_lower(paste(cur_name_split[[1]][2], 
                            cur_name_split[[1]][1])))
  } else {
    return(str_to_lower(cur_name))
  }
}
rename_effects <- Vectorize(rename_effects)

unique_effects <- nrow(effects_clean %>% filter(use_effect))

freq_exposures <-  effects_clean %>% 
  group_by(plain_language_exposure) %>% 
  summarise(n=n()) %>% 
  arrange(desc(n)) %>% 
  mutate(plain_language_exposure = rename_effects(plain_language_exposure),
         plain_language_exposure = case_when(
           plain_language_exposure=="lifestyle risk behaviour (at school) intervention" ~"screen-based lifestyle risk behaviour interventions (at school)",
           TRUE ~ plain_language_exposure),
         out = paste0(plain_language_exposure, " (n = ", n, ")"))

freq_outcomes <- effects_clean %>% 
  group_by(plain_language_outcome) %>% 
  summarise(n=n()) %>% 
  arrange(desc(n)) %>%   
  mutate(plain_language_outcome = rename_effects(plain_language_outcome),
         out = paste0(plain_language_outcome, " (n = ", n, ")"))

freq_combos <- effects_clean %>% 
  group_by(plain_language_exposure, plain_language_outcome) %>% 
  summarise(n=n()) %>% 
  arrange(desc(n))
```

The searches yielded `r format(prisma$data$value[1], big.mark=",")` results, of which `r format(prisma$data$value[2], big.mark=",")` were duplicates. After screening titles and abstracts, we assessed `r format(prisma$data$value[5], big.mark=",")` full-texts for inclusion. `r str_to_sentence(numbers_to_words(prisma$data$value[8], and=TRUE))` met the inclusion criteria and we extracted the data of these meta-analyses. Figure \@ref(fig:prisma) presents the full results of the selection process.

```{r prisma, fig.cap="PRISMA Diagram"}
prisma$diag
```

After extracting data, we examined the combinations of exposure and outcomes and removed any effects that appeared more than once, keeping the effect with the largest total sample size. This process yielded `r unique_effects` unique effects/outcome combinations contributed from `r prisma$data$value[9]` papers. 

The most frequently reported exposures were `r knitr::combine_words(freq_exposures$out[1:4])`. The most frequently reported outcomes were `r knitr::combine_words(freq_outcomes$out[1:5])`. In most cases (`r paste0(sum(freq_combos$n==1), "/", nrow(effects_clean))`), there was only one exposure/outcome combination, with `r sum(freq_combos$n==2)` appearing twice, and `r sum(freq_combos$n>2)` three or more times. Full characteristics of the included studies are provided in Table \@ref(tab:desctable).

```{r desctable, layout="l-screen-inset shaded"}
reviews_tables[[1]]$table
```

```{r quality}
qual_data <- 
  tables_df %>%
  filter(table=="main") %>% 
  distinct(covidence_id, .keep_all = TRUE) %>% 
  select(eligibility_criteria_predefined_and_specified:heterogeneity_assessed)

reviews_count <- qual_data %>%
  mutate(
    across(
      eligibility_criteria_predefined_and_specified:heterogeneity_assessed,
      as.factor
    )
  ) %>%
  rowwise() %>%
  mutate(count_low = sum(
    c_across(
      eligibility_criteria_predefined_and_specified:heterogeneity_assessed
    ) == "low"
  ))

all_low <- reviews_count %>%
  filter(count_low==7) %>% 
  nrow()

reviews_count <-
  reviews_count %>%
  select(eligibility_criteria_predefined_and_specified:heterogeneity_assessed) %>%
  gather(name, value) %>%
  count(name, value) %>%
  arrange(desc(n))

qual_str <- function(n, totaln, ntype="", prefix="(n", suffix=")"){
  paste0(prefix,ntype,"= ",n,"/",totaln,", ", scales::percent(n/totaln, accuracy = 1), suffix)
}

n_low_hetero <-
  reviews_count[reviews_count$name == "heterogeneity_assessed" &
                  reviews_count$value == "low", "n"][[1]]
n_low_char <-
  reviews_count[reviews_count$name == "included_studies_listed_with_important_characteristics_and_results_of_each" &
                  reviews_count$value == "low", "n"][[1]]
n_low_search <-
  reviews_count[reviews_count$name == "literature_search_strategy_comprehensive_and_systematic" &
                  reviews_count$value == "low", "n"][[1]]

n_unclear_elig <-
  reviews_count[reviews_count$name == "eligibility_criteria_predefined_and_specified" &
                  reviews_count$value == "unclear", "n"][[1]]

n_high_dualscreen <-
  reviews_count[reviews_count$name == "dual_independent_screening_review" &
                  reviews_count$value == "high", "n"][[1]]
n_unclear_dualscreen <-
  reviews_count[reviews_count$name == "dual_independent_screening_review" &
                  reviews_count$value == "unclear", "n"][[1]]

n_high_dualquality <-
  reviews_count[reviews_count$name == "dual_independent_quality_assessment" &
                  reviews_count$value == "high", "n"][[1]]
n_unclear_dualquality <-
  reviews_count[reviews_count$name == "dual_independent_quality_assessment" &
                  reviews_count$value == "unclear", "n"][[1]]

dualqualstr <- paste0(
  qual_str(n_high_dualquality, nrow(qual_data), " high ", suffix= "; "),
  qual_str(n_unclear_dualquality, nrow(qual_data), " unclear ", prefix = "n", suffix = ")"))


```

The quality of the included meta-analyses was mixed. Most assessed heterogeneity `r qual_str(n_low_hetero, nrow(qual_data), " low ", suffix=" of studies)")`, reported the characteristics of the included studies `r qual_str(n_low_char, nrow(qual_data), " low ")`, and used a comprehensive and systematic search strategy `r qual_str(n_low_search, nrow(qual_data), " low ")`. Most reviews did not clearly report if their eligibility criteria was predefined `r qual_str(n_unclear_elig, nrow(qual_data), " unclear ")`. Many reviews also did not complete dual independent screening of abstracts and full text `r qual_str(n_high_dualscreen, nrow(qual_data), " high ")` or did not clearly report the method of screening `r qual_str(n_unclear_dualscreen, nrow(qual_data), " unclear ")`. Many reviews  also did not complete A similar trend was observed for dual independent quality assessment `r dualqualstr`. Overall, only `r all_low` meta-analyses were graded as *low* risk of bias on all criteria.

### Education Outcomes

```{r eduoutcomes}
edu_effect <-   
  combined_effects %>%
  filter(outcome_category=="education" & use_effect)

edu_certain <-
  combined_effects %>%
  filter(outcome_category == "education" & certainty == "meets criteria")

n_edu_effect <-
  edu_effect %>%
  nrow()

n_edu_certain <-
  edu_certain %>% 
  nrow()

n_edu_noindiv <- 
  edu_effect %>% 
  filter(source == "reported") %>% 
  nrow()

n_edu_samplesize <- 
  edu_effect %>%
  filter(source == "reanalysis" & n < 1000) %>% 
  nrow()

n_edu_faileggers <- 
  edu_effect %>%
  filter(source == "reanalysis" & n > 1000 & 
           (eggers_p < 0.05 | is.na(eggers_p))) %>% 
  nrow()

n_edu_failtest <- 
  edu_effect %>% 
  filter(source == "reanalysis" & n > 1000 & eggers_p > 0.05 & tes_p < 0.05) %>% 
  nrow()
```


There were `r n_edu_effect` unique effects associated with education outcomes, including general learning outcomes, literacy, numeracy, and science. Of these, `r n_edu_certain` met our criteria for statistical credibility and are described below (see Figure \@ref(fig:eduplot)). We removed `r n_edu_noindiv` for not providing individual study-level data, `r n_edu_samplesize` for having meta-analytic effects with samples < 1,000, and `r n_edu_faileggers` had a significant Egger's test or insufficient studies to conduct the test. No remaining studies showed evidence of excessive significance. Effects not meeting one or more of these standards are presented in the supplementary material.

```{r eduplot, fig.width=plots[[1]]$dims[[1]], fig.height=plots[[1]]$dims[[2]], fig.cap="Education outcomes"}
plots[[1]]$plot
```


Among the statistically credible effects, general screen use, TV viewing, and video games were all negatively associated with academic performance or general learning. E-books that included narration, as well as touch screen education interventions, and augmented reality interventions were positively associated with learning. General screen use was also negatively associated with literacy outcomes. However, if the screen involved co-viewing (e.g., watching with a parent) or the content was educational the association with literacy was positive. Numeracy outcomes were positively associated with screen-based mathematics interventions and video games with numeracy content.

```{r educertain}
n_edu_p999 <- edu_certain %>% filter(reanalysis_p.value<0.001) %>% nrow()
n_edu_p95 <- edu_certain %>% filter(reanalysis_p.value<0.05) %>% nrow() - n_edu_p999

n_edu_i2 <- edu_certain %>% filter(i2 > 50) %>% nrow()
```


As shown in Figure \@ref(fig:eduplot), most of the credible results (`r n_edu_p999` of `r n_edu_certain` effects) showed statistically significant associations, with 99.9% confidence intervals not encompassing zero (strong evidence). The remaining `r xfun::numbers_to_words(n_edu_p95)` associations were significant at the 95% confidence level (weak evidence). All credible effects related to education outcomes were small-to-moderate. Screen-based interventions tended to have larger effect sizes than non-intervention exposure (e.g., TV viewing, e-book reading) with the largest effect size observed for augmented reality-based school interventions on general learning ($r = 0.33$). Most effects showed high levels of heterogeneity (`r n_edu_i2` of `r n_edu_certain` with $I^2 > 50\%$).

### Health and Health-related Behaviours

```{r healthoutcome}
health_effect <-   
  combined_effects %>%
  filter(outcome_category != "education" & use_effect)

health_certain <-
  combined_effects %>%
  filter(outcome_category != "education" & certainty == "meets criteria")

n_health_effect <-
  health_effect %>%
  nrow()

n_health_certain <-
  health_certain %>% 
  nrow()

n_health_noindiv <- 
  health_effect %>% 
  filter(source == "reported") %>% 
  nrow()

n_health_samplesize <- 
  health_effect %>%
  filter(source == "reanalysis" & n < 1000) %>% 
  nrow()

n_health_faileggers <- 
  health_effect %>%
  filter(source == "reanalysis" & n > 1000 & 
           (eggers_p < 0.05 | is.na(eggers_p))) %>% 
  nrow()

n_health_failtest <- 
  health_effect %>% 
  filter(source == "reanalysis" & n > 1000 & eggers_p > 0.05 & tes_p < 0.05) %>% 
  nrow()
```

We identified `r n_health_effect` unique outcome-exposure combinations associated with health or health-related behaviour outcomes. `r xfun::numbers_to_words(n_health_certain, cap=TRUE)` of these meta-analytic associations met our criteria for credible evidence (see Figure \@ref(fig:healthplot)). We removed `r n_health_noindiv` for not providing individual study-level data, `r n_health_samplesize` for having meta-analytic effects with samples < 1,000, and `r n_health_faileggers` had a significant Egger's test or insufficient studies to conduct the test. No remaining studies showed evidence of excessive significance. Effects not meeting one or more of these standards are presented in the supplementary material. 

```{r healthplot, fig.width=plots[[2]]$dims[[1]], fig.height=plots[[2]]$dims[[2]], fig.cap="Health and health-related behaviour outcomes"}
plots[[2]]$plot
```

Advertising of unhealthy foods—both traditional advertising and video games developed by a brand for promotion—were associated with higher food intake. Social media use was associated with higher risk of substance abuse, while screen-based interventions which target health behaviours appeared effective. 

Television was negatively correlated with sleep duration, but only at the 95% confidence level (weak evidence). All forms of observed screen time (general, TV, and video games) were positively associated with body composition (e.g., BMI). Risky behaviour (e.g., unsafe sex) was also positively associated with social media and with the use of sexy media. General screen use was also associated with depression.

```{r healthcertain}
n_health_p999 <- health_certain %>% filter(reanalysis_p.value<0.001) %>% nrow()
n_health_p95 <- health_certain %>% filter(reanalysis_p.value<0.05) %>% nrow() - n_edu_p999

n_health_i2 <- health_certain %>% filter(i2 > 50) %>% nrow()
n_health_r2 <- health_certain %>% filter(abs(r) < 0.2) %>% nrow()
```

Across the health outcomes, most (`r n_health_p999` of `r n_health_certain` effects) were statistically significant at the $P < 10^3$ level, with the remaining `r xfun::numbers_to_words(n_health_p95)` significant at the $P < .05$ level. However, all of the credible effects exhibited high levels of heterogeneity, with the lowest $I^2 = 75%$. Additionally, most effects were small, with the association between unhealthy food advertising and food intake the largest at $r = 0.25$. Most of the effect sizes (`r n_health_r2`/`r n_health_certain`) had an absolute value of $r < 0.2$. 

# Discussion

**Note:** *Below are rough notes on ideas for the discussion. Please feel free to add ideas. We will revise once we agree on the rest of the paper.*

#### Broad summary of the findings relating back to the purpose
When meta-analysis examined general screen use, and did not specify the content, context or device, there was strong evidence showing potentially harmful associations with academic performance, literacy, body composition, and depression. However, when meta-analyses included a more nuanced examination of exposures a more complex picture appeared. Many exposures were likely neither universally harmful nor consistently beneficial. 

#### TV
Strong evidence indicated that watching television programs was associated with poorer academic performance and learning. However, there was also weak evidence that if the child watched these programs with a parent (i.e., coviewing) or the content of the program was educational then television viewing was associated with better literacy.
 
Outside of educational outcomes, there was strong evidence of a harmful association with body composition and marginal associations with sleep duration. Overall, there was credible evidence indicating that TV watching was likely to confer benefits for children, but it may not be harmful as long as children watch with their parents or the content is educational.

#### Video games 
* Weak evidence of harm for body comp.
* Strong evidence of harm for academic performance.
* Strong evidence of benefit for numeracy games on numeracy.

#### Screen-based Interventions
One category of exposure appeared to consistently confer benefits: Interventions delivered via screens. This finding indicates that interventions can be effectively delivered using electronic media platforms. It does not necessarily indicate that screens are more effective than other methods (e.g., face-to-face, printed material). 

* Touch screen = strong evidence of benefit for learning
* Math interventions = strong evidence on numeracy.
* Augmented reality on learning, strong evidence.
* Screen-based interventions. Strong effect on health behaviours.

#### Consistent Harms
The exposures that were consistently associated with harms: social media and food advertising via screen media.  
Social media showed strong evidence of harmful associations with risk taking in general, as well as unsafe sex and substance abuse.  
Strong evidence also indicated screen-based food advertising (in general and in advergames), was risky as it was positively associated with food intake.  
Finally, there was strong evidence that sexy media was associated with sexual activity.

#### Implications of findings for targeted policy and practice
Screen time is complex. We need to stop thinking about it in simplistic terms as “bad” and something that should be minimised in all instances. Need to think about how it can best be used. For example, recreational television and video games appear to confer few, if any benefits, and appear to be associated with poor body composition and academic performance. However, tv programs and video games designed to promote learning may be effective. The current literature indicates that social media is likely harmful, but if social media platforms specifically designed to promote learning can be designed and popularised, perhaps they could have benefits.

#### Survey of the field and implications for future research

Screen time research is big and varied and rapidly growing. Reviews tended to be quite general (e.g., all screentime) and even when more targeted (e.g., social media) nuances related to specific content (e.g., Instagram vs Facebook) have not been meta-analysed.

There is credible evidence for X, we need more evidence for Y.  
All observational research with self-report evidence.

#### Limitations of the review

Can’t look at fine-grained detail - eg moderators, but we did look at different exposures and outcomes (quasi moderators).  
GRADE - we did not rate the risk of bias of individual studies in the meta-analyses. Total k  >3,000 (intractable task)  
We made decisions to present effects that reach certain thresholds (e.g., N>1,000, non-significant Egger’s test) but also provide more complete results in the supplementary files for those who wish to investigate.

#### Conclusions

# Appendix

#### Figures

* [Prisma Diagram](https://drive.google.com/file/d/1ezgeAr2WPhy6MNSESS8bhg3VMOrLAUC7/view?usp=sharing)
* [Education Outcomes](https://drive.google.com/file/d/1ODsKf3jjEGNCpUTz5tju8AHRpk_xIz-e/view?usp=sharing)
* [Health-Related Outcomes](https://drive.google.com/file/d/1wdKzliWLEFuVveAj6-MzOAsoqBhnkRJW/view?usp=sharing)
* [Supplemental Education Outcomes](https://drive.google.com/file/d/1wUotZRh-jvdBJh5dkstkMtvcz7DPn-vS/view?usp=sharing)
* [Supplemental Health-Related Outcomes](https://drive.google.com/file/d/1-3frrZ3woWjPqkO84Dn3_1CCaE8thIZn/view?usp=sharing)

#### Tables


#### Links

* [Manuscript Google Doc](https://docs.google.com/document/d/1NN4yJWQVepNjbX0mND4aidWWXcCDkg-eMepRnWdtQ9o/edit#)
* [Online Report](https://motivation-and-behaviour.github.io/screen_umbrella/)

```{r refs, results='asis'}
if (params$inc_ref) "# References"
```
